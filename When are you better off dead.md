If a hermit dies in the forest and no one ever hears about it, is the world better off? 

To be precise, let us say that this hermit leads a life of suffering, and you could decide to painlessly kill him. Would do it? Now what if I said that this hermit doesn’t exist, but you could conjure him into existence. Would you do that?

It seems that from a consequentialist point of view, these acts should be equivalent. But most people would intuit that they are not. Even if you answered that you *would* kill him, could you imagine a level of suffering where you *would* spare him but you *wouldn’t* conjure him into existence? 

I suspect that for most people, they would have a higher threshold of suffering for killing the child than they would have for not conjuring the child into existence. These people can’t be consequentialists, because even though the end result of *killing* and *not conjuring* is the same, they wouldn’t kill even though they wouldn’t conjure. 

But with consequentialism, if all else is equal, there should be no asymmetry. And it is perhaps possible to make a case in this direction. A consequentialist might say that our intuition fails us here because in the vast majority of cases where we hesitate to take a life when we would easily refrain from giving one because all else *isn’t* equal. If someone is already alive, people know them. People love them. Killing them them would cause suffering to all those people, even if it were a painless death. Not to mention, killing someone would cause anguish to whoever ends up doing the killing. On the other hand, no one is in love with the twinkle in your eye that may one day become a person if you so decide.

But that is why I give the example above about the hermit who can be killed or conjured without any further repercussions[^0]. If in that case you see no asymmetry, you are a dyed-in-the-wool consequentialist, and this article is for you (if you do see an asymmetry, you are already a non-consequentialist, but feel free to read on for fun).

Let us assume you think that taking and not giving a life is the same when all else is equal. This means we have to come up with a zero point: the point below which someone is better off dead. If you couldn’t do this, then we’d be forced into the conclusion that it is always better to increase the number of people without limit, because any life would be better than no life[^1]. 

Where would you put this zero point? At the boundary between happy and sad? Surely life is more valuable than that. To quote Louis CK, “even a shitty, shitty life is worth living, apparently, because folks are living the fuck out of them.”

Which raises another possible zero point — perhaps it is the point at which you would commit suicide. I don’t think we (as a society) really think this, though, or we would not go to such ends to prevent suicide. Perhaps you could make the case that people are being irrational when they are committing suicide, and if they could really be rational and weigh their options, accounting for all the future benefit they are foregoing, that would make an ideal zero point[^2]. 

This, and any other proposal for a zero point, has problems. Whichever zero point you set, it would imply that we should add people as long as their happiness is above the zero point. But is a universe of three billion very happy people really worse than a universe of six billion people, where we’ve added (with affecting the initial three billion) another three billion who are doing just well enough not to be better off dead?   

Consequentialism has no choice but to say yes. Because if we start with the second universe, the only (consequentialist) justification for *not* killing off the three billion people living in misery is that it must make the world worse. But I think many people might disagree with this, and say that, actually, three billion very happy people is much better, on the whole. 

This becomes relevant whenever we try to judge alternative futures with different numbers of people. By the argument above, consequentialists have no choice but to say that, as long as people’s well-being is above the zero point, more is better. But I think many people would intuitively disagree. They might say, who really cares if it’s 300 million, 3 billion, or 30 billion people, if the average happiness is the highest at 300 million, make it 300 million. 

This has tremendous practical implications. For instance, should we invest in technologies that would enable us to be an interplanetary society? Or should we just have fewer babies? The consequentialist might be inclined to say yes to the first and no to the second, in which case he should judge Elon Musk as the paragon of virtue (at least in these two respects).

The problem comes up in other contexts, too. You might think, for instance, that the world would be better if everyone were a vegetarian. But in such a world, the population of domesticated chickens would go from 26 billion to approximately zero. Now, you might think that the chickens alive today are living in such squalor that they are below the zero point, and a world without them would be better. Fair enough. But certainly, there should be some level of humaneness where the chickens would be better off alive. Is the ideal world the one in which there is enough meat consumption to maintain such a humanely raised population of chickens?

[^0]:Well, technically, there is one repercussion: the hypothetical killer remembers killing the hermit, and may suffer remorse. But first of all, if the killer really did improve the world, why should they suffer remorse? A true consequentialist should not suffer this way if they killed the hermit. And second, it is possible to construct the example where the killing causes no suffering, because the killer is, for instance, a sociopath, or an unfeeling AI system.
[^1]:This of course would require that the new people you add don’t affect the lives of the existing people. But this, of course, can be done in principle.
[^2]:There are strong cases for *both* the claim that that is too low, and it is too high. First, to argue it is too high: if someone rationally comes to the decision to kill himself, he must decide while he is still alive. If he is afraid of death, it means he is committing to a period between his decision and his death of intense and existential anguish. So, to rationally decide to kill yourself, you not only have to think you are better off dead, but you must think that you are better of dying a death that will be preceded by a period of intense anguish, which is of course worse than dying a painless death. And to argue it is too low, think: suppose you could conjure into existence a person who is living a terrible life, but just a hair above rationally deciding to commit suicide. Would you do it?